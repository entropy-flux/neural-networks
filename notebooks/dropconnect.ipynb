{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bfd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0656826",
   "metadata": {},
   "source": [
    "\n",
    "$$ \n",
    "\\text{DropConnect}(X, W, M) = \\begin{bmatrix}\n",
    "    \\frac{1}{1-p}\\begin{bmatrix} x^1{}_1 & x^1{}_2 & \\cdots & x^1{}_d \\end{bmatrix}\n",
    "    \\left(\\begin{bmatrix}\n",
    "        m^{11}{}_1 & m^{11}{}_2 & \\cdots & m^{11}{}_l \\\\\n",
    "        m^{12}{}_1 & m^{12}{}_2 & \\cdots & m^{12}{}_l \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        m^{1d}{}_1 & m^{1d}{}_2 & \\cdots & m^{1d}{}_l \\\\\n",
    "    \\end{bmatrix} \\odot \\begin{bmatrix}\n",
    "        w^1{}_1 & w^1{}_2 & \\cdots & w^1{}_l \\\\\n",
    "        w^2{}_1 & w^2{}_2 & \\cdots & w^2{}_l \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w^d{}_1 & w^d{}_2 & \\cdots & w^d{}_l \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\right) \\\\\n",
    "    \\\\\n",
    "    \\frac{1}{1-p}\\begin{bmatrix}  x^2{}_1 & x^2{}_2 & \\cdots & x^2{}_d \\end{bmatrix}\n",
    "    \\left(\\begin{bmatrix}\n",
    "        m^{21}{}_1 & m^{21}{}_2 & \\cdots & m^{21}{}_l \\\\\n",
    "        m^{22}{}_1 & m^{22}{}_2 & \\cdots & m^{22}{}_l \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        m^{2d}{}_1 & m^{2d}{}_2 & \\cdots & m^{2d}{}_l \\\\\n",
    "    \\end{bmatrix} \\odot \\begin{bmatrix}\n",
    "        w^1{}_1 & w^1{}_2 & \\cdots & w^1{}_l \\\\\n",
    "        w^2{}_1 & w^2{}_2 & \\cdots & w^2{}_l \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w^d{}_1 & w^d{}_2 & \\cdots & w^d{}_l \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\right) \\\\\n",
    "    \\\\\n",
    "    \\vdots\n",
    "    \\\\\n",
    "    \\\\\n",
    "    \\frac{1}{1-p}\\begin{bmatrix}  x^n{}_1 & x^n{}_2 & \\cdots & x^n{}_d \\end{bmatrix}\n",
    "    \\left(\\begin{bmatrix}\n",
    "        m^{n1}{}_1 & m^{n1}{}_2 & \\cdots & m^{n1}{}_l \\\\\n",
    "        m^{n2}{}_1 & m^{n2}{}_2 & \\cdots & m^{n2}{}_l \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        m^{nd}{}_1 & m^{nd}{}_2 & \\cdots & m^{nd}{}_l \\\\\n",
    "    \\end{bmatrix} \\odot \\begin{bmatrix}\n",
    "        w^1{}_1 & w^1{}_2 & \\cdots & w^1{}_l \\\\\n",
    "        w^2{}_1 & w^2{}_2 & \\cdots & w^2{}_l \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w^d{}_1 & w^d{}_2 & \\cdots & w^d{}_l \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\right) \\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916e7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torcheval.metrics import Mean, MulticlassAccuracy\n",
    " \n",
    "class Metrics:\n",
    "    def __init__(self, device: str | None = None):\n",
    "        self.loss = Mean(device=device)\n",
    "        self.accuracy = MulticlassAccuracy(num_classes=10, device=device) \n",
    "        \n",
    "    def update(self, batch: int, loss: Tensor, predictions: Tensor, targets: Tensor) -> None:\n",
    "        self.loss.update(loss)\n",
    "        self.accuracy.update(predictions, targets)\n",
    "        if batch % 200 == 0:\n",
    "            print(f\"--- Batch {batch}: loss={loss.item()}\")\n",
    "        \n",
    "    def compute(self) -> dict[str, Tensor]:\n",
    "        return {\n",
    "            'loss': self.loss.compute(),\n",
    "            'accuracy': self.accuracy.compute()\n",
    "        }\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.loss.reset()\n",
    "        self.accuracy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afff4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import argmax\n",
    "from torch.nn import Module, Flatten\n",
    "from torch.optim import Optimizer\n",
    "from torchsystem import Aggregate\n",
    "\n",
    "class Classifier(Aggregate):\n",
    "    def __init__(self, hash: str, model: Module, criterion: Module, optimizer: Optimizer, metrics: Metrics):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        self.flatten = Flatten()\n",
    "        self.hash = hash\n",
    "        self.epoch = 0\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return self.hash\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self.model(self.flatten(input))\n",
    "    \n",
    "    def loss(self, outputs: Tensor, targets: Tensor) -> Tensor:\n",
    "        return self.criterion(outputs, targets)\n",
    "\n",
    "    def fit(self, inputs: Tensor, targets: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return argmax(outputs, dim=1), loss\n",
    "    \n",
    "    def evaluate(self, inputs: Tensor, targets: Tensor) -> tuple[Tensor, Tensor]: \n",
    "        outputs = self(inputs)\n",
    "        return argmax(outputs, dim=1), self.loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ee632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable \n",
    "from torchsystem.depends import Depends, Provider\n",
    "from torchsystem.services import Service, Consumer, event\n",
    "from mltracker import getallmodels\n",
    "from mltracker.ports import Models\n",
    "\n",
    "provider = Provider()\n",
    "consumer = Consumer(provider=provider) \n",
    "service = Service(provider=provider)\n",
    "\n",
    "def device() -> str:...\n",
    "\n",
    "def models() -> Models:...\n",
    "\n",
    "@service.handler\n",
    "def train(model: Classifier, loader: Iterable[tuple[Tensor, Tensor]], device: str = Depends(device)):\n",
    "    model.phase = 'train'\n",
    "    for batch, (inputs, targets) in enumerate(loader, start=1): \n",
    "        inputs, targets = inputs.to(device), targets.to(device)  \n",
    "        predictions, loss = model.fit(inputs, targets)\n",
    "        model.metrics.update(batch, loss, predictions, targets)\n",
    "    results = model.metrics.compute()\n",
    "    consumer.consume(Trained(model, results))\n",
    "\n",
    "@event\n",
    "class Trained:\n",
    "    model: Classifier \n",
    "    results: dict[str, Tensor]\n",
    "\n",
    "\n",
    "@consumer.handler\n",
    "def handle_epoch(event: Trained):\n",
    "    event.model.epoch += 1\n",
    "\n",
    "@consumer.handler\n",
    "def handle_results(event: Trained, models: Models = Depends(models)):\n",
    "    model = models.read(event.model.id)\n",
    "    for name, metric in event.results.items():\n",
    "        model.metrics.add(name, metric.item(), event.model.epoch, event.model.phase)\n",
    "\n",
    "@consumer.handler\n",
    "def print_metrics(event: Trained):\n",
    "    print(f\"-----------------------------------------------------------------\")\n",
    "    print(f\"Epoch: {event.model.epoch}, Average loss: {event.results['loss'].item()}, Average accuracy: {event.results['accuracy'].item()}\")\n",
    "    print(f\"-----------------------------------------------------------------\")\n",
    "\n",
    "@consumer.handler\n",
    "def save_epoch(event: Trained, models: Models = Depends(models)):\n",
    "    model = models.read(event.model.id) or models.create(event.model.id)\n",
    "    model.epoch = event.model.epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e24a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsystem import Depends\n",
    "from torchsystem.compiler import Compiler, compile\n",
    "\n",
    "compiler = Compiler[Classifier](provider=provider)\n",
    "\n",
    "@compiler.step\n",
    "def build_model(nn: Module, criterion: Module, optimizer: Module, metrics: Metrics, device: str = Depends(device)):\n",
    "    print(f\"Moving classifier to device {device}...\")\n",
    "    metrics.accuracy.to(device)\n",
    "    metrics.loss.to(device)\n",
    "    return Classifier('1', nn, criterion, optimizer, metrics).to(device)\n",
    "\n",
    "@compiler.step\n",
    "def compile_model(classifier: Classifier):\n",
    "    print(\"Compiling model...\")\n",
    "    return compile(classifier)\n",
    "\n",
    "@compiler.step\n",
    "def bring_to_current_epoch(classifier: Classifier, models: Models = Depends(models)):\n",
    "    print(\"Retrieving model from store...\")\n",
    "    model = models.read(classifier.id)\n",
    "    if not model:\n",
    "        print(f\"model not found, creating one...\")\n",
    "        model = models.create(classifier.id, 'classifier')\n",
    "    else:\n",
    "        print(f\"model found on epoch {model.epoch}\")\n",
    "    classifier.epoch = model.epoch\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceac18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import Tensor, bernoulli, full_like, empty\n",
    "from torch.nn import init, Module, Linear, Parameter, ReLU\n",
    "from torch.nn.functional import linear\n",
    "\n",
    "def dropconnect(features: Tensor, weight: Tensor, bias: Tensor | None = None, p: float = 0.5) -> Tensor:\n",
    "    assert 0 <= p < 1, f\"DropConnect probability p must be in [0, 1), got {p}\"\n",
    "    device = features.device\n",
    "    # Shared mask across the batch\n",
    "    mask = bernoulli(full_like(weight, 1 - p)).bool()\n",
    "    masked_weight = weight.masked_fill(~mask, 0) / (1 - p)\n",
    "    return linear(features, masked_weight, bias)\n",
    "\n",
    "class Dropconnect(Module):\n",
    "    __constants__ = [\"in_features\", \"out_features\", \"p\"]\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, p: float = 0.5, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, features: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            return dropconnect(features, self.weight, self.bias, self.p)\n",
    "        else:\n",
    "            return linear(features, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}, p={self.p}\"\n",
    "\n",
    "class DCP(Module):\n",
    "    def __init__(self, input_features: int, hidden_features: int, output_features: int, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_layer = Dropconnect(input_features, hidden_features, p=p)\n",
    "        self.activation = ReLU()\n",
    "        self.output_layer = Linear(hidden_features, output_features)\n",
    "\n",
    "    def forward(self, features: Tensor) -> Tensor:\n",
    "        features = self.input_layer(features)\n",
    "        features = self.activation(features)\n",
    "        return self.output_layer(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdb743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving classifier to device cuda:0...\n",
      "Compiling model...\n",
      "Retrieving model from store...\n",
      "model not found, creating one...\n",
      "--- Batch 200: loss=0.2356145679950714\n",
      "--- Batch 400: loss=0.24830473959445953\n",
      "--- Batch 600: loss=0.09275729954242706\n",
      "--- Batch 800: loss=0.11138495057821274\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 1, Average loss: 0.23751319520699699, Average accuracy: 0.929183304309845\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.10199681669473648\n",
      "--- Batch 400: loss=0.19998936355113983\n",
      "--- Batch 600: loss=0.0863189697265625\n",
      "--- Batch 800: loss=0.12480195611715317\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 2, Average loss: 0.17145907957561568, Average accuracy: 0.9485833048820496\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.09888274222612381\n",
      "--- Batch 400: loss=0.11287199705839157\n",
      "--- Batch 600: loss=0.024831566959619522\n",
      "--- Batch 800: loss=0.03278230503201485\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 3, Average loss: 0.13970092471123705, Average accuracy: 0.9577833414077759\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.13592803478240967\n",
      "--- Batch 400: loss=0.014902623370289803\n",
      "--- Batch 600: loss=0.014693111181259155\n",
      "--- Batch 800: loss=0.012947289273142815\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 4, Average loss: 0.1195529286540163, Average accuracy: 0.9635416865348816\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.045000772923231125\n",
      "--- Batch 400: loss=0.06654511392116547\n",
      "--- Batch 600: loss=0.050887711346149445\n",
      "--- Batch 800: loss=0.015857990831136703\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 5, Average loss: 0.10565534026233175, Average accuracy: 0.9675999879837036\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.011847905814647675\n",
      "--- Batch 400: loss=0.006704396568238735\n",
      "--- Batch 600: loss=0.0296319592744112\n",
      "--- Batch 800: loss=0.06656543910503387\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 6, Average loss: 0.09483604264317687, Average accuracy: 0.970769464969635\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.12078633159399033\n",
      "--- Batch 400: loss=0.010622885078191757\n",
      "--- Batch 600: loss=0.031423572450876236\n",
      "--- Batch 800: loss=0.0763014703989029\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 7, Average loss: 0.08689563445328258, Average accuracy: 0.9730666875839233\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.012409658171236515\n",
      "--- Batch 400: loss=0.0072443000972270966\n",
      "--- Batch 600: loss=0.0198153555393219\n",
      "--- Batch 800: loss=0.013519853353500366\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 8, Average loss: 0.08043960964145443, Average accuracy: 0.9749249815940857\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.07998330891132355\n",
      "--- Batch 400: loss=0.0020224612671881914\n",
      "--- Batch 600: loss=0.00523240165784955\n",
      "--- Batch 800: loss=0.08103024959564209\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 9, Average loss: 0.07494074554852219, Average accuracy: 0.9765203595161438\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.03304874151945114\n",
      "--- Batch 400: loss=0.00508989067748189\n",
      "--- Batch 600: loss=0.1958051323890686\n",
      "--- Batch 800: loss=0.05184570699930191\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 10, Average loss: 0.0701759055274312, Average accuracy: 0.9779850244522095\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from datasets.digits import Digits\n",
    "\n",
    "repository = getallmodels('dropconnect')\n",
    "provider.override(device, lambda: 'cuda:0')\n",
    "provider.override(models, lambda: repository) \n",
    "\n",
    "nn = DCP(784, 256, 10, p=0.2)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(nn.parameters(), lr=0.001)\n",
    "metrics = Metrics()\n",
    "classifier = compiler.compile(nn, criterion, optimizer, metrics)\n",
    "datasets = {\n",
    "    'train': Digits(train=True, transform=Compose([ToTensor(), Normalize(0.1307, 0.3081)])),\n",
    "    'evaluation': Digits(train=True, transform=Compose([ToTensor(), Normalize(0.1307, 0.3081)])),\n",
    "}\n",
    "loaders = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=64, shuffle=True, pin_memory=True, pin_memory_device='cuda', num_workers=4),\n",
    "    'evaluation': DataLoader(datasets['evaluation'], batch_size=64, shuffle=True, pin_memory=True, pin_memory_device='cuda', num_workers=4)\n",
    "}\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(classifier, loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be355ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving classifier to device cuda:0...\n",
      "Compiling model...\n",
      "Retrieving model from store...\n",
      "model not found, creating one...\n",
      "--- Batch 200: loss=0.2838340997695923\n",
      "--- Batch 400: loss=0.35911861062049866\n",
      "--- Batch 600: loss=0.20519579946994781\n",
      "--- Batch 800: loss=0.1655597984790802\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 1, Average loss: 0.2550781427253125, Average accuracy: 0.9235333204269409\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.18443703651428223\n",
      "--- Batch 400: loss=0.10197896510362625\n",
      "--- Batch 600: loss=0.11046610027551651\n",
      "--- Batch 800: loss=0.13306830823421478\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 2, Average loss: 0.18731357665089116, Average accuracy: 0.9436833262443542\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.03697671741247177\n",
      "--- Batch 400: loss=0.009128664620220661\n",
      "--- Batch 600: loss=0.12625662982463837\n",
      "--- Batch 800: loss=0.13148345053195953\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 3, Average loss: 0.1549907946689736, Average accuracy: 0.9531055688858032\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.01983419992029667\n",
      "--- Batch 400: loss=0.05916272848844528\n",
      "--- Batch 600: loss=0.02581198886036873\n",
      "--- Batch 800: loss=0.07936456054449081\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 4, Average loss: 0.13425185459108452, Average accuracy: 0.9592750072479248\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.025673402473330498\n",
      "--- Batch 400: loss=0.060988884419202805\n",
      "--- Batch 600: loss=0.12034635245800018\n",
      "--- Batch 800: loss=0.02346174605190754\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 5, Average loss: 0.11959099901710023, Average accuracy: 0.9636200070381165\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.10542339086532593\n",
      "--- Batch 400: loss=0.006078357342630625\n",
      "--- Batch 600: loss=0.03543790057301521\n",
      "--- Batch 800: loss=0.014463790692389011\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 6, Average loss: 0.10843881826536524, Average accuracy: 0.9667860865592957\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.029136216267943382\n",
      "--- Batch 400: loss=0.13726140558719635\n",
      "--- Batch 600: loss=0.026543203741312027\n",
      "--- Batch 800: loss=0.03983175754547119\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 7, Average loss: 0.0997755233464142, Average accuracy: 0.9693642854690552\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.07462488859891891\n",
      "--- Batch 400: loss=0.07586804777383804\n",
      "--- Batch 600: loss=0.008113468997180462\n",
      "--- Batch 800: loss=0.036442819982767105\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 8, Average loss: 0.092698838934957, Average accuracy: 0.9714541435241699\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.014660428278148174\n",
      "--- Batch 400: loss=0.0035438728518784046\n",
      "--- Batch 600: loss=0.01665981113910675\n",
      "--- Batch 800: loss=0.018914518877863884\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 9, Average loss: 0.08677687322336779, Average accuracy: 0.9731444716453552\n",
      "-----------------------------------------------------------------\n",
      "--- Batch 200: loss=0.04514951631426811\n",
      "--- Batch 400: loss=0.08153042197227478\n",
      "--- Batch 600: loss=0.014752131886780262\n",
      "--- Batch 800: loss=0.09704620391130447\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 10, Average loss: 0.08166372958528645, Average accuracy: 0.9746783375740051\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from datasets.digits import Digits\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, input_features: int, hidden_features: int, output_features: int, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.epoch = 0\n",
    "        self.input_layer = Linear(input_features, hidden_features)\n",
    "        self.dropout = Dropout(p)\n",
    "        self.activation = ReLU()\n",
    "        self.output_layer = Linear(hidden_features, output_features)\n",
    "\n",
    "    def forward(self, features: Tensor) -> Tensor:\n",
    "        features = self.input_layer(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.activation(features)\n",
    "        return self.output_layer(features)\n",
    "\n",
    "repository = getallmodels('dropconnect')\n",
    "provider.override(device, lambda: 'cuda:0')\n",
    "provider.override(models, lambda: repository) \n",
    "\n",
    "nn = MLP(784, 256, 10, 0.2)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(nn.parameters(), lr=0.001)\n",
    "metrics = Metrics()\n",
    "classifier = compiler.compile(nn, criterion, optimizer, metrics)\n",
    "datasets = {\n",
    "    'train': Digits(train=True, transform=Compose([ToTensor(), Normalize(0.1307, 0.3081)])),\n",
    "    'evaluation': Digits(train=True, transform=Compose([ToTensor(), Normalize(0.1307, 0.3081)])),\n",
    "}\n",
    "loaders = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=64, shuffle=True, pin_memory=True, pin_memory_device='cuda', num_workers=4),\n",
    "    'evaluation': DataLoader(datasets['evaluation'], batch_size=64, shuffle=True, pin_memory=True, pin_memory_device='cuda', num_workers=4)\n",
    "}\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(classifier, loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb1dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f59b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921a0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
